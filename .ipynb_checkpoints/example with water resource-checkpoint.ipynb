{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sujawon\").master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----+--------+\n",
      "|번호|            시간|우량|누적우량|\n",
      "+----+----------------+----+--------+\n",
      "|   1|2021-02-22 01:00|   0|    29.5|\n",
      "|   2|2021-02-22 02:00|   0|    29.5|\n",
      "|   3|2021-02-22 03:00|   0|    29.5|\n",
      "|   4|2021-02-22 04:00|   0|    29.5|\n",
      "|   5|2021-02-22 05:00|   0|    29.5|\n",
      "|   6|2021-02-22 06:00|   0|    29.5|\n",
      "|   7|2021-02-22 07:00|   0|    29.5|\n",
      "|   8|2021-02-22 08:00|   0|    29.5|\n",
      "|   9|2021-02-22 09:00|   0|    29.5|\n",
      "+----+----------------+----+--------+\n",
      "\n",
      "+-------+------+---+----------------+\n",
      "|acmtlrf|hourrf| no|          obsrdt|\n",
      "+-------+------+---+----------------+\n",
      "|   29.5|     0|  1|2021-02-22 01:00|\n",
      "|   29.5|     0|  2|2021-02-22 02:00|\n",
      "|   29.5|     0|  3|2021-02-22 03:00|\n",
      "|   29.5|     0|  4|2021-02-22 04:00|\n",
      "|   29.5|     0|  5|2021-02-22 05:00|\n",
      "|   29.5|     0|  6|2021-02-22 06:00|\n",
      "|   29.5|     0|  7|2021-02-22 07:00|\n",
      "|   29.5|     0|  8|2021-02-22 08:00|\n",
      "|   29.5|     0|  9|2021-02-22 09:00|\n",
      "+-------+------+---+----------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'> <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- 번호: integer (nullable = true)\n",
      " |-- 시간: string (nullable = true)\n",
      " |-- 우량: integer (nullable = true)\n",
      " |-- 누적우량: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- acmtlrf: string (nullable = true)\n",
      " |-- hourrf: string (nullable = true)\n",
      " |-- no: string (nullable = true)\n",
      " |-- obsrdt: string (nullable = true)\n",
      "\n",
      "+----+----------------+----+--------+\n",
      "|번호|            시간|우량|누적우량|\n",
      "+----+----------------+----+--------+\n",
      "|   1|2021-02-22 01:00|   0|    29.5|\n",
      "|   2|2021-02-22 02:00|   0|    29.5|\n",
      "|   3|2021-02-22 03:00|   0|    29.5|\n",
      "|   4|2021-02-22 04:00|   0|    29.5|\n",
      "|   5|2021-02-22 05:00|   0|    29.5|\n",
      "|   6|2021-02-22 06:00|   0|    29.5|\n",
      "|   7|2021-02-22 07:00|   0|    29.5|\n",
      "|   8|2021-02-22 08:00|   0|    29.5|\n",
      "|   9|2021-02-22 09:00|   0|    29.5|\n",
      "+----+----------------+----+--------+\n",
      "\n",
      "<class 'NoneType'>\n",
      "root\n",
      " |-- 번호: string (nullable = true)\n",
      " |-- 시간: string (nullable = true)\n",
      " |-- 우량: string (nullable = true)\n",
      " |-- 누적우량: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_시간별_2021-02-22.csv\")\n",
    "df2 = spark.read.format(\"json\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_분별_2021-02-22.json\")\n",
    "\n",
    "df.show()\n",
    "df2.show()\n",
    "print(type(df),type(df2))\n",
    "df.printSchema()\n",
    "df2.printSchema()\n",
    "\n",
    "#df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_시간별_2021-02-22.csv\").show()로 만들경우 nonetype으로 정의\n",
    "df3 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_시간별_2021-02-22.csv\").show()\n",
    "print(type(df3))\n",
    "\n",
    "df4 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"false\").load(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_시간별_2021-02-22.csv\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dFtable\")\n",
    "df2.createOrReplaceTempView(\"dF2table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----+--------+\n",
      "|번호|            시간|우량|누적우량|\n",
      "+----+----------------+----+--------+\n",
      "|   1|2021-02-22 01:00|   0|    29.5|\n",
      "|   2|2021-02-22 02:00|   0|    29.5|\n",
      "|   3|2021-02-22 03:00|   0|    29.5|\n",
      "|   4|2021-02-22 04:00|   0|    29.5|\n",
      "|   5|2021-02-22 05:00|   0|    29.5|\n",
      "|   6|2021-02-22 06:00|   0|    29.5|\n",
      "|   7|2021-02-22 07:00|   0|    29.5|\n",
      "|   8|2021-02-22 08:00|   0|    29.5|\n",
      "|   9|2021-02-22 09:00|   0|    29.5|\n",
      "+----+----------------+----+--------+\n",
      "\n",
      "+-------+------+---+----------------+\n",
      "|acmtlrf|hourrf| no|          obsrdt|\n",
      "+-------+------+---+----------------+\n",
      "|   29.5|     0|  1|2021-02-22 01:00|\n",
      "|   29.5|     0|  2|2021-02-22 02:00|\n",
      "|   29.5|     0|  3|2021-02-22 03:00|\n",
      "|   29.5|     0|  4|2021-02-22 04:00|\n",
      "|   29.5|     0|  5|2021-02-22 05:00|\n",
      "|   29.5|     0|  6|2021-02-22 06:00|\n",
      "|   29.5|     0|  7|2021-02-22 07:00|\n",
      "|   29.5|     0|  8|2021-02-22 08:00|\n",
      "|   29.5|     0|  9|2021-02-22 09:00|\n",
      "+-------+------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsql = spark.sql(\"SELECT * FROM dFtable\")\n",
    "df2sql = spark.sql(\"SELECT * FROM dF2table\")\n",
    "\n",
    "dfsql.show()\n",
    "df2sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----+--------+\n",
      "|번호|            시간|우량|누적우량|\n",
      "+----+----------------+----+--------+\n",
      "|   1|2021-02-22 01:00|   0|    29.5|\n",
      "|   2|2021-02-22 02:00|   0|    29.5|\n",
      "|   3|2021-02-22 03:00|   0|    29.5|\n",
      "|   4|2021-02-22 04:00|   0|    29.5|\n",
      "|   5|2021-02-22 05:00|   0|    29.5|\n",
      "|   6|2021-02-22 06:00|   0|    29.5|\n",
      "|   7|2021-02-22 07:00|   0|    29.5|\n",
      "|   8|2021-02-22 08:00|   0|    29.5|\n",
      "|   9|2021-02-22 09:00|   0|    29.5|\n",
      "+----+----------------+----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['번호', '시간', '우량', '누적우량']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"*\").show()\n",
    "df.select(\"*\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        | df2table|       true|\n",
      "|        |  dftable|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = spark.sql(\"SHOW DATABASES\")\n",
    "db2 = spark.sql(\"SHOW TABLES\")\n",
    "\n",
    "db.show()\n",
    "db2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df.withColumnRenamed(\"번호\",\"num\",).withColumnRenamed(\"시간\",\"time\",).withColumnRenamed(\"우량\",\"prec\",).withColumnRenamed(\"누적우량\",\"nestedprec\",).createOrReplaceTempView(\"engdFtable\")\n",
    "#한글로 명명된 열 이름을 영어로 변환시켜 테이블로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----+----------+\n",
      "|num|            time|prec|nestedprec|\n",
      "+---+----------------+----+----------+\n",
      "|  1|2021-02-22 01:00|   0|      29.5|\n",
      "|  2|2021-02-22 02:00|   0|      29.5|\n",
      "|  3|2021-02-22 03:00|   0|      29.5|\n",
      "|  4|2021-02-22 04:00|   0|      29.5|\n",
      "|  5|2021-02-22 05:00|   0|      29.5|\n",
      "|  6|2021-02-22 06:00|   0|      29.5|\n",
      "|  7|2021-02-22 07:00|   0|      29.5|\n",
      "|  8|2021-02-22 08:00|   0|      29.5|\n",
      "|  9|2021-02-22 09:00|   0|      29.5|\n",
      "+---+----------------+----+----------+\n",
      "\n",
      "+---+----------------+\n",
      "|num|            time|\n",
      "+---+----------------+\n",
      "|  1|2021-02-22 01:00|\n",
      "|  2|2021-02-22 02:00|\n",
      "|  3|2021-02-22 03:00|\n",
      "|  4|2021-02-22 04:00|\n",
      "|  5|2021-02-22 05:00|\n",
      "|  6|2021-02-22 06:00|\n",
      "|  7|2021-02-22 07:00|\n",
      "|  8|2021-02-22 08:00|\n",
      "|  9|2021-02-22 09:00|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM engdFtable\").show()\n",
    "#영어로 변환된 열이름의 테이블이 출력\n",
    "spark.sql(\"SELECT num,time FROM engdFtable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|total daily prec|average prec per hour|\n",
      "+----------------+---------------------+\n",
      "|           265.5|                  0.0|\n",
      "+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sum(nestedprec) as `total daily prec` ,avg(prec) as `average prec per hour` FROM engdFtable\").show()\n",
    "#하루의 누적강수량과 시간당 평균 강수량을 보여주는 sql문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['번호,시간,우량,누적우량',\n",
       " '1,2021-02-22 01:00,0,29.5',\n",
       " '2,2021-02-22 02:00,0,29.5',\n",
       " '3,2021-02-22 03:00,0,29.5',\n",
       " '4,2021-02-22 04:00,0,29.5',\n",
       " '5,2021-02-22 05:00,0,29.5',\n",
       " '6,2021-02-22 06:00,0,29.5',\n",
       " '7,2021-02-22 07:00,0,29.5',\n",
       " '8,2021-02-22 08:00,0,29.5',\n",
       " '9,2021-02-22 09:00,0,29.5']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csv파일을 RDD로 만든후 연산이후에 dataframe으로 바꾸고 임시뷰테이블로 전환하는 과정\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "lines = sc.textFile(\"C:/Users/b2en/Desktop/spark관련/수자원공사_우량관측정보_시간별_2021-02-22.csv\")\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['번호', '시간', '우량', '누적우량'],\n",
       " ['1', '2021-02-22 01:00', '0', '29.5'],\n",
       " ['2', '2021-02-22 02:00', '0', '29.5'],\n",
       " ['3', '2021-02-22 03:00', '0', '29.5'],\n",
       " ['4', '2021-02-22 04:00', '0', '29.5'],\n",
       " ['5', '2021-02-22 05:00', '0', '29.5'],\n",
       " ['6', '2021-02-22 06:00', '0', '29.5'],\n",
       " ['7', '2021-02-22 07:00', '0', '29.5'],\n",
       " ['8', '2021-02-22 08:00', '0', '29.5'],\n",
       " ['9', '2021-02-22 09:00', '0', '29.5']]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(누적우량='누적우량', 번호='번호', 시간='시간', 우량='우량'),\n",
       " Row(누적우량='29.5', 번호='1', 시간='2021-02-22 01:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='2', 시간='2021-02-22 02:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='3', 시간='2021-02-22 03:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='4', 시간='2021-02-22 04:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='5', 시간='2021-02-22 05:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='6', 시간='2021-02-22 06:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='7', 시간='2021-02-22 07:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='8', 시간='2021-02-22 08:00', 우량='0'),\n",
       " Row(누적우량='29.5', 번호='9', 시간='2021-02-22 09:00', 우량='0')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = parts.map(lambda p: Row(번호=p[0], 시간=(p[1]), 우량=p[2], 누적우량=p[3]))\n",
    "people.collect()\n",
    "\n",
    "#collect()\tReturn all the elements of the dataset as an array at the driver program.\n",
    "#This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\n",
    "\n",
    "#select()는 transformation lazy한 spark특징상. collect()는 action.으로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------------+----+\n",
      "|누적우량|번호|            시간|우량|\n",
      "+--------+----+----------------+----+\n",
      "|누적우량|번호|            시간|우량|\n",
      "|    29.5|   1|2021-02-22 01:00|   0|\n",
      "|    29.5|   2|2021-02-22 02:00|   0|\n",
      "|    29.5|   3|2021-02-22 03:00|   0|\n",
      "|    29.5|   4|2021-02-22 04:00|   0|\n",
      "|    29.5|   5|2021-02-22 05:00|   0|\n",
      "|    29.5|   6|2021-02-22 06:00|   0|\n",
      "|    29.5|   7|2021-02-22 07:00|   0|\n",
      "|    29.5|   8|2021-02-22 08:00|   0|\n",
      "|    29.5|   9|2021-02-22 09:00|   0|\n",
      "+--------+----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemapeople = spark.createDataFrame(people)\n",
    "schemapeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(schemapeople))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|        |  df2table|       true|\n",
      "|        |   dftable|       true|\n",
      "|        |engdftable|       true|\n",
      "|        |    people|       true|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemapeople.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------------+----+\n",
      "|누적우량|번호|            시간|우량|\n",
      "+--------+----+----------------+----+\n",
      "|누적우량|번호|            시간|우량|\n",
      "|    29.5|   1|2021-02-22 01:00|   0|\n",
      "|    29.5|   2|2021-02-22 02:00|   0|\n",
      "|    29.5|   3|2021-02-22 03:00|   0|\n",
      "|    29.5|   4|2021-02-22 04:00|   0|\n",
      "|    29.5|   5|2021-02-22 05:00|   0|\n",
      "|    29.5|   6|2021-02-22 06:00|   0|\n",
      "|    29.5|   7|2021-02-22 07:00|   0|\n",
      "|    29.5|   8|2021-02-22 08:00|   0|\n",
      "|    29.5|   9|2021-02-22 09:00|   0|\n",
      "+--------+----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
