{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3335efe8b96d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0msample_df2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"store\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"product\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"amount\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"price\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mldf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mrdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word' is not defined"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "from word import Word\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///Users/beginspark/Temp/\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 파이썬에서 데이터프레임 생성 시 네임드튜플(namedtuple), 튜플(tuple)\n",
    "# Row, 커스텀 클래스(class), 딕셔너리(dictionary) 등을\n",
    "# 사용하여 생성할 수 있다\n",
    "Person = collections.namedtuple('Person', 'name age job')\n",
    "\n",
    "# sample dataframe 1\n",
    "row1 = Person(name=\"hayoon\", age=7, job=\"student\")\n",
    "row2 = Person(name=\"sunwoo\", age=13, job=\"student\")\n",
    "row3 = Person(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "row4 = Person(name=\"jinwoo\", age=13, job=\"student\")\n",
    "data = [row1, row2, row3, row4]\n",
    "sample_df = spark.createDataFrame(data)\n",
    "\n",
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "sample_df2 = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "\n",
    "ldf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w2\", 1)])\n",
    "rdf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w3\", 1)])\n",
    "\n",
    "\n",
    "# createDataFrame\n",
    "def createDataFrame(spark, sc):\n",
    "    sparkHomeDir = \"file:/Users/beginspark/Apps/spark\"\n",
    "\n",
    "    # 1. 파일로 부터 생성\n",
    "    df1 = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df2 = spark.read.parquet(sparkHomeDir + \"/examples/src/main/resources/users.parquet\")\n",
    "    df3 = spark.read.text(sparkHomeDir + \"/examples/src/main/resources/people.txt\")\n",
    "\n",
    "    # 2. 컬렉션으로부터 생성 (ex5-17)\n",
    "    row1 = Row(name=\"hayoon\", age=7, job=\"student\")\n",
    "    row2 = Row(name=\"sunwoo\", age=13, job=\"student\")\n",
    "    row3 = Row(name=\"hajoo\", age=5, job=\"kindergartener\")\n",
    "    row4 = Row(name=\"jinwoo\", age=13, job=\"student\")\n",
    "    data = [row1, row2, row3, row4]\n",
    "    df4 = spark.createDataFrame(data)\n",
    "\n",
    "    # 3. RDD로부터 생성 (ex5-20)\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    df5 = spark.createDataFrame(data)\n",
    "\n",
    "    # 4. 스키마 지정(ex5-23)\n",
    "    sf1 = StructField(\"name\", StringType(), True)\n",
    "    sf2 = StructField(\"age\", IntegerType(), True)\n",
    "    sf3 = StructField(\"job\", StringType(), True)\n",
    "    schema = StructType([sf1, sf2, sf3])\n",
    "    r1 = (\"hayoon\", 7, \"student\")\n",
    "    r2 = (\"sunwoo\", 13, \"student\")\n",
    "    r3 = (\"hajoo\", 5, \"kindergartener\")\n",
    "    r4 = (\"jinwoo\", 13, \"student\")\n",
    "    rows = [r1, r2, r3, r4]\n",
    "    df6 = spark.createDataFrame(rows, schema)\n",
    "\n",
    "\n",
    "# 5.5.2.1.1절 ~ 5.5.2.2.4절\n",
    "def runBasicOpsEx(spark, sc, df):\n",
    "    df.show()\n",
    "    df.head()\n",
    "    df.first()\n",
    "    df.take(2)\n",
    "    df.count()\n",
    "    df.collect()\n",
    "    df.describe(\"age\").show()\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "    df.printSchema()\n",
    "    df.columns\n",
    "    df.dtypes\n",
    "    df.schema\n",
    "    df.createOrReplaceTempView(\"users\")\n",
    "    spark.sql(\"select name, age from users where age > 20\").show()\n",
    "    spark.sql(\"select name, age from users where age > 20\").explain()\n",
    "\n",
    "\n",
    "# 5.5.2.4절\n",
    "def runColumnEx(spark, sc, df):\n",
    "    df.where(df.age > 10).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.2절\n",
    "def runAlias(spark, sc, df):\n",
    "    df.select(df.age + 1).show()\n",
    "    df.select((df.age + 1).alias(\"age\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.3절\n",
    "def runIsinEx(spark, sc):\n",
    "    nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "    rdd = spark.sparkContext.parallelize(range(0, 10)).map(lambda v: Row(v))\n",
    "    df = spark.createDataFrame(rdd)\n",
    "    df.where(df._1.isin(nums.value)).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.4절\n",
    "def runWhenEx(spark, sc):\n",
    "    ds = spark.range(0, 5)\n",
    "    col = when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "    ds.select(ds.id, col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.5절\n",
    "def runMaxMin(spark, df):\n",
    "    min_col = min(\"age\")\n",
    "    max_col = max(\"age\")\n",
    "    df.select(min_col, max_col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.6절 ~ 5.5.2.4.9절\n",
    "def runAggregateFunctions(spark, df1, df2):\n",
    "    # collect_list, collect_set\n",
    "    doubledDf1 = df1.union(df1)\n",
    "    doubledDf1.select(functions.collect_list(doubledDf1[\"name\"])).show(truncate=False)\n",
    "    doubledDf1.select(functions.collect_set(doubledDf1[\"name\"])).show(truncate=False)\n",
    "\n",
    "    # count, countDistinct\n",
    "    doubledDf1.select(functions.count(doubledDf1[\"name\"]), functions.countDistinct(doubledDf1[\"name\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "    # sum\n",
    "    df2.printSchema()\n",
    "    df2.select(sum(df2[\"price\"])).show(truncate=False)\n",
    "\n",
    "    # grouping, grouping_id\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping(df2[\"store\"])).show(truncate=False)\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping_id(df2[\"store\"], df2[\"product\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.12 ~ 5.5.2.4.14절\n",
    "def runDateFunctions(spark):\n",
    "    f1 = StructField(\"d1\", StringType(), True)\n",
    "    f2 = StructField(\"d2\", StringType(), True)\n",
    "    schema1 = StructType([f1, f2])\n",
    "\n",
    "    df = spark.createDataFrame([(\"2017-12-25 12:00:05\", \"2017-12-25\")], schema1)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # current_date, unix_timestamp, to_date\n",
    "    d3 = current_date().alias(\"d3\")\n",
    "    d4 = unix_timestamp(df[\"d1\"].alias(\"d4\"))\n",
    "    d5 = to_date(df[\"d2\"].alias(\"d5\"))\n",
    "    d6 = to_date(d4.cast(\"timestamp\")).alias(\"d6\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d3, d4, d5, d6).show(truncate=False)\n",
    "\n",
    "    # add_months, date_add, last_day\n",
    "    d7 = add_months(d6, 2).alias(\"d7\")\n",
    "    d8 = date_add(d6, 2).alias(\"d8\")\n",
    "    d9 = last_day(d6).alias(\"d9\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d7, d8, d9).show(truncate=False)\n",
    "\n",
    "    # window\n",
    "    f3 = StructField(\"date\", StringType(), True)\n",
    "    f4 = StructField(\"product\", StringType(), True)\n",
    "    f5 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema2 = StructType([f3, f4, f5])\n",
    "\n",
    "    r2 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    r3 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    r4 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    r5 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    r6 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    r7 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    r8 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    r9 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([r2, r3, r4, r5, r6, r7, r8, r9], schema2);\n",
    "\n",
    "    timeCol = unix_timestamp(dd[\"date\"]).cast(\"timestamp\");\n",
    "    windowCol = window(timeCol, \"5 minutes\");\n",
    "    dd.groupBy(windowCol, dd[\"product\"]).agg(sum(dd[\"amount\"])).show(truncate=False);\n",
    "\n",
    "\n",
    "# 5.5.2.4.15절\n",
    "def runDateFunctions(spark):\n",
    "    # 파이썬의 경우 아래와 같이 튜플을 이용하여 데이터프레임을 생성하는 것도 가능함\n",
    "    df1 = spark.createDataFrame([(1.512,), (2.234,), (3.42,)], ['value'])\n",
    "    df2 = spark.createDataFrame([(25.0,), (9.0,), (10.0,)], ['value'])\n",
    "\n",
    "    df1.select(round(df1[\"value\"], 1)).show()\n",
    "    df2.select(functions.sqrt('value')).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.16 ~ 5.5.2.4.20절\n",
    "def runOtherFunctions(spark, personDf):\n",
    "    df = spark.createDataFrame([(\"v1\", \"v2\", \"v3\")], [\"c1\", \"c2\", \"c3\"]);\n",
    "\n",
    "    # array\n",
    "    df.select(df.c1, df.c2, df.c3, array(\"c1\", \"c2\", \"c3\").alias(\"newCol\")).show(truncate=False)\n",
    "\n",
    "    # desc, asc\n",
    "    personDf.show()\n",
    "    personDf.sort(functions.desc(\"age\"), functions.asc(\"name\")).show()\n",
    "\n",
    "    # pyspark 2.1.0 버전은 desc_nulls_first, desc_nulls_last, asc_nulls_first, asc_nulls_last 지원하지 않음\n",
    "\n",
    "    # split, length (pyspark에서 컬럼은 df[\"col\"] 또는 df.col 형태로 사용 가능)\n",
    "    df2 = spark.createDataFrame([(\"Splits str around pattern\",)], ['value'])\n",
    "    df2.select(df2.value, split(df2.value, \" \"), length(df2.value)).show(truncate=False)\n",
    "\n",
    "    # rownum, rank\n",
    "    f1 = StructField(\"date\", StringType(), True)\n",
    "    f2 = StructField(\"product\", StringType(), True)\n",
    "    f3 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema = StructType([f1, f2, f3])\n",
    "\n",
    "    p1 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    p2 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    p3 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    p4 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    p5 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    p6 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    p7 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    p8 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([p1, p2, p3, p4, p5, p6, p7, p8], schema)\n",
    "    w1 = Window.partitionBy(\"product\").orderBy(\"amount\")\n",
    "    w2 = Window.orderBy(\"amount\")\n",
    "    dd.select(dd.product, dd.amount, functions.row_number().over(w1).alias(\"rownum\"),\n",
    "              functions.rank().over(w2).alias(\"rank\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.21절\n",
    "def runUDF(spark, df):\n",
    "    # functions를 이용한 등록\n",
    "    fn1 = functions.udf(lambda job: job == \"student\")\n",
    "    df.select(df[\"name\"], df[\"age\"], df[\"job\"], fn1(df[\"job\"])).show()\n",
    "    # SparkSession을 이용한 등록\n",
    "    spark.udf.register(\"fn2\", lambda job: job == \"student\")\n",
    "    df.createOrReplaceTempView(\"persons\")\n",
    "    spark.sql(\"select name, age, job, fn2(job) from persons\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.24절\n",
    "def runAgg(spark, df):\n",
    "    df.agg(max(\"amount\"), min(\"price\")).show()\n",
    "    df.agg({\"amount\": \"max\", \"price\": \"min\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.26절\n",
    "def runDfAlias(spark, df):\n",
    "    df.select(df[\"product\"]).show()\n",
    "    df.alias(\"aa\").select(\"aa.product\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.27절\n",
    "def runGroupBy(spark, df):\n",
    "    df.groupBy(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.3.4.28절\n",
    "def runCube(spark, df):\n",
    "    df.cube(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.29절\n",
    "def runDistinct(spark):\n",
    "    d1 = (\"store1\", \"note\", 20, 2000)\n",
    "    d2 = (\"store1\", \"bag\", 10, 5000)\n",
    "    d3 = (\"store1\", \"note\", 20, 2000)\n",
    "    rows = [d1, d2, d3]\n",
    "    cols = [\"store\", \"product\", \"amount\", \"price\"]\n",
    "    df = spark.createDataFrame(rows, cols)\n",
    "    df.distinct().show()\n",
    "    df.dropDuplicates([\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.30절\n",
    "def runDrop(spark, df):\n",
    "    df.drop(df[\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.31절\n",
    "def runIntersect(spark):\n",
    "    a = spark.range(1, 5)\n",
    "    b = spark.range(2, 6)\n",
    "    c = a.intersect(b)\n",
    "    c.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.32절\n",
    "def runExcept(spark):\n",
    "    df1 = spark.range(1, 6)\n",
    "    df2 = spark.createDataFrame([(2,), (4,)], ['value'])\n",
    "    # 파이썬의 경우 except 대신 subtract 메서드 사용\n",
    "    # subtract의 동작은 except와 같음\n",
    "    df1.subtract(df2).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.33절\n",
    "def runJoin(spark, ldf, rdf):\n",
    "    joinTypes = \"inner,outer,leftouter,rightouter,leftsemi\".split(\",\")\n",
    "    for joinType in joinTypes:\n",
    "        print(\"============= %s ===============\" % joinType)\n",
    "        ldf.join(rdf, [\"word\"], joinType).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.35절\n",
    "def runNa(spark, ldf, rdf):\n",
    "    result = ldf.join(rdf, [\"word\"], \"outer\").toDF(\"word\", \"c1\", \"c2\")\n",
    "    result.show()\n",
    "    # 파이썬의 경우 na.drop또는 dropna 사용 가능\n",
    "    # c1과 c2 칼럼의 null이 아닌 값의 개수가 thresh 이하일 경우 drop\n",
    "    # thresh=1로 설정할 경우 c1 또는 c2 둘 중의 하나만 null 아닌 값을 가질 경우\n",
    "    # 결과에 포함시킨다는 의미가 됨\n",
    "    result.na.drop(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    result.dropna(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    # fill\n",
    "    result.na.fill({\"c1\": 0}).show()\n",
    "    # 파이썬의 경우 to_replace에 딕셔너리를 지정하여 replace를 수행(이 경우 value에 선언한 값은 무시됨\n",
    "    # 딕셔너리를 사용하지 않을 경우 키 목록(첫번째 인자)과 값 목록(두번째 인자)을 지정하여 replace 수행\n",
    "    result.na.replace(to_replace={\"w1\": \"word1\", \"w2\": \"word2\"}, value=\"\", subset=\"word\").show()\n",
    "    result.na.replace([\"w1\", \"w2\"], [\"word1\", \"word2\"], \"word\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.36절\n",
    "def runOrderBy(spark):\n",
    "    df = spark.createDataFrame([(3, \"z\"), (10, \"a\"), (5, \"c\")], [\"idx\", \"name\"])\n",
    "    df.orderBy(\"name\", \"idx\").show()\n",
    "    df.orderBy(\"idx\", \"name\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.37절\n",
    "def runRollup(spark, df):\n",
    "    df.rollup(\"store\", \"product\").agg({\"price\": \"sum\"}).show();\n",
    "\n",
    "\n",
    "# 5.5.2.4.38절\n",
    "def runStat(spark):\n",
    "    df = spark.createDataFrame([(\"a\", 6), (\"b\", 4), (\"c\", 12), (\"d\", 6)], [\"word\", \"count\"])\n",
    "    df.show()\n",
    "    df.stat.crosstab(\"word\", \"count\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.39절\n",
    "def runWithColumn(spark):\n",
    "    df1 = spark.createDataFrame([(\"prod1\", \"100\"), (\"prod2\", \"200\")], [\"pname\", \"price\"])\n",
    "    df2 = df1.withColumn(\"dcprice\", df1[\"price\"] * 0.9)\n",
    "    df3 = df2.withColumnRenamed(\"dcprice\", \"newprice\")\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "    df3.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.40절\n",
    "def runSave(spark):\n",
    "    sparkHomeDir = \"file:///Users/beginspark/Apps/spark\"\n",
    "    df = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df.write.save(\"/Users/beginspark/Temp/default/%d\" % time.time())\n",
    "    df.write.format(\"json\").save(\"/Users/beginspark/Temp/json/%d\" % time.time())\n",
    "    df.write.format(\"json\").partitionBy(\"age\").save(\"/Users/beginspark/Temp/parti/%d\" % time.time())\n",
    "    # saveMode: append, overwrite, error, ignore\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"ohMyTable\")\n",
    "    spark.sql(\"select * from ohMyTable\").show()\n",
    "    # 파이썬의 경우 bucketBy 지원하지 않음\n",
    "\n",
    "# [예제 실행 방법] 아래에서 원하는 예제의 주석을 제거하고 실행!!\n",
    "\n",
    "# runBasicOpsEx(spark, sc, sample_df)\n",
    "# createDataFrame(spark, sc)\n",
    "# runColumnEx(spark, sc, sample_df)\n",
    "# runAlias(spark, sc, sample_df)\n",
    "# runIsinEx(spark, sc)\n",
    "# runWhenEx(spark, sc)\n",
    "# runMaxMin(spark, sample_df)\n",
    "# runAggregateFunctions(spark, sample_df, sample_df2)\n",
    "# runCollectionFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runOtherFunctions(spark, sample_df)\n",
    "# runUDF(spark, sample_df)\n",
    "# runAgg(spark, sample_df2)\n",
    "# runDfAlias(spark, sample_df2)\n",
    "# runGroupBy(spark, sample_df2)\n",
    "# runCube(spark, sample_df2)\n",
    "# runDistinct(spark)\n",
    "# runDrop(spark, sample_df2)\n",
    "# runIntersect(spark)\n",
    "# runExcept(spark)\n",
    "# runJoin(spark, ldf, rdf)\n",
    "# runNa(spark, ldf, rdf)\n",
    "# runOrderBy(spark)\n",
    "# runRollup(spark, sample_df2)\n",
    "# runWithColumn(spark)\n",
    "# runSave(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
